{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This notebook borrows heavily from the tutorial notebook for the code used in Gerlach et al. (2018), which is available at https://github.com/martingerlach/hSBM_Topicmodel. Please ensure that their code base sbmtm.py is downloaded to the same folder as the notebook and that the graph-tool package (https://graph-tool.skewed.de/) is intalled on your kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import sys \n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import pylab as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import graph_tool.all as gt\n",
    "from sbmtm import sbmtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data\n",
    "\n",
    "The code below scrapes and imports the data from PhilPapers.org. To re-import in a way that collects all of the most recent abstracts in Formal Epistemology, change \"limit=2803\" in the URL below to \"limit=x\", where x is the total number of formal epistemology papers listed on PhilPapers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data.\n",
    "URL = \"https://philpapers.org/browse/formal-epistemology?cn=formal-epistemology&freeOnly=&proOnly=on&cId=5467&langFilter=&hideAbstracts=&showCategories=off&filterByAreas=&categorizerOn=&new=1&limit=2803&start=0&sort=cat&onlineOnly=&publishedOnly=&sqc=&newWindow=&format=html&jlist=&ap_c1=&ap_c2=\"\n",
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "entries = soup.find_all(\"li\", class_='entry')\n",
    "\n",
    "#Set n to the total number of formal epistemology papers.\n",
    "n = 2803\n",
    "\n",
    "titles = [0]*n\n",
    "authors = [0]*n\n",
    "years = [0]*n\n",
    "publications = [0]*n\n",
    "abstracts = [0]*n\n",
    "\n",
    "#Separate the abstracts, titles, authors, years, and publications for each paper. \n",
    "for i in range(0,n):\n",
    "    abstracts[i]=entries[i].find_all(\"div\", class_=\"abstract\")\n",
    "    titles[i] = entries[i].find_all(\"span\", class_='articleTitle recTitle')\n",
    "    authors[i] = entries[i].find_all(\"span\", class_='name')\n",
    "    years[i] = entries[i].find_all(\"span\", class_='pubYear')\n",
    "    publications[i] = entries[i].find_all(\"em\", class_='pubName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the Data\n",
    "------------------\n",
    "\n",
    "The code below cleans the data. After this step, I also manually removed brackets and quotation marks from the .txt file containing the abstracts. This created a number of instances of the text token ' s ', which I also removed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape the arrays for all data points.\n",
    "titles = np.array(titles, dtype=object).reshape(-1,1)\n",
    "authors = np.array(authors, dtype=object).reshape(-1,1)\n",
    "years = np.array(years, dtype=object).reshape(-1,1)\n",
    "publications = np.array(publications, dtype=object).reshape(-1,1)\n",
    "abstracts = np.array(abstracts, dtype=object).reshape(-1,1)\n",
    "\n",
    "#Replace papers with empty titles or abstracts with zeros.\n",
    "for i in range(0,len(abstracts)):\n",
    "    if len(abstracts[i][0])==0:\n",
    "        abstracts[i]=0\n",
    "    else:\n",
    "        abstracts[i]=abstracts[i]\n",
    "for i in range(0,len(abstracts)):\n",
    "    if titles[i] in titles[0:i]:\n",
    "        titles[i]=0\n",
    "    else:\n",
    "        titles[i]=titles[i]\n",
    "        \n",
    "#Combine all data arrays into a full array, and remove all entries with empty abstacts or titles.\n",
    "full_array=np.hstack((titles,authors,years,publications,abstracts))\n",
    "full_array=np.delete(full_array,np.where(full_array[:,4]==0),0)\n",
    "full_array=np.delete(full_array,np.where(full_array[:,0]==0),0)\n",
    "\n",
    "\n",
    "#Separate out all data arrays.\n",
    "titles=full_array[:,0]\n",
    "authors=full_array[:,1]\n",
    "years=full_array[:,2]\n",
    "publications=full_array[:,3]\n",
    "abstracts=full_array[:,4]\n",
    "\n",
    "\n",
    "#Remove stopwords and HTML tags from abstracts.\n",
    "all_stopwords = stopwords.words('english')\n",
    "other_removals = ['epistemology','formal','paper','article','chapter',',','.','...','shrink',':',';','!','(',')','\"','“','”','``','1','2','3','4','5','6','7','8','9','0','sections','`','dropbox','google','cambridge','kindle', '’', 'argue','argued','argument','argues','new','first','also','must','propose','proposal','claim','claimed','claims','work','framework','-','--','---','oxford','doogle','https','doi:','{','}','$','?','-/-','two','three','four','five','six','seven','eight','nine','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','\\\\\\\\usepackage','@','``','“','”','‘','–','>','<','\\\\\\\\cS','arguments','argument','accounts','account','problem','theory','theories','one','show','reasoning','way','may','given','case','analysis','study','whether','cases','make','result','recent','would','upon','debate','key','describe','since','use','used','using','suggest','either','problem','different','approach','based','problems','part','various','proposed','shown','concern','concerned','particular','present','general','provide','many','literature','us','several','sense','provides','ways','process','approaches','within','discuss','main','best','therefore','review','proposes','incorporating','famous','considerable','please','send','drive','university','students']\n",
    "all_stopwords = np.append(all_stopwords,other_removals)\n",
    "for i in range(0,len(abstracts)):\n",
    "    abstracts[i][0] = BeautifulSoup(str(abstracts[i][0]), \"lxml\").text\n",
    "    abstracts[i][0] = word_tokenize(str(abstracts[i][0]))\n",
    "    abstracts[i][0] = [word for word in abstracts[i][0] if not word.lower() in all_stopwords]\n",
    "    abstracts[i][0] = (\" \").join(abstracts[i][0])\n",
    "    \n",
    "    \n",
    "#Save abstracts as a .txt file. \n",
    "np.savetxt('abstracts.txt',abstracts,fmt='%s')\n",
    "with open('abstracts.txt') as f:\n",
    "    html_cleaned_abstracts = BeautifulSoup(f, \"lxml\").text\n",
    "with open('abstracts.txt', 'w') as output_file:\n",
    "    output_file.write(html_cleaned_abstracts)\n",
    "    \n",
    "    \n",
    "#Prepare abstracts and titles for analysis by the sbtm model.\n",
    "fname_data = 'abstracts.txt'\n",
    "filename = os.path.join(fname_data)\n",
    "with open(filename,'r', encoding = 'utf8') as f:\n",
    "    x = f.readlines()\n",
    "texts = [h.split() for h in x]\n",
    "documents = [str(titles[i]) for i in range(0,len(titles))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model\n",
    "-------------\n",
    "\n",
    "The code below infers the heirarchical stochastic block model for the abstracts, producing partition(s) of word and document nodes at multiple levels of abstraction. For the sake of the analysis in my paper, I focus solely on the partitions of the word nodes. For n=2809 inference typically takes less than five minutes, but may take longer depending on your machine, and can take much longer for larger n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we create an instance of the sbmtm-class\n",
    "model = sbmtm()\n",
    "\n",
    "## we have to create the word-document network from the corpus\n",
    "model.make_graph(texts,documents)\n",
    "\n",
    "## we can also skip the previous step by saving/loading a graph\n",
    "# model.save_graph(filename = 'graph.xml.gz')\n",
    "# model.load_graph(filename = 'graph.xml.gz')\n",
    "\n",
    "## fit the model\n",
    "gt.seed_rng(32) ## seed for graph-tool's random number generator --> same results\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results\n",
    "-------------\n",
    "\n",
    "The code below extracts the data used in the paper and saves them as .txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the topics as levels 1 and 2 as .txt files.\n",
    "with open('level1_topics.txt', 'w') as f:\n",
    "    print(model.topics(l=1,n=20), file=f)\n",
    "with open('level2_topics.txt', 'w') as f:\n",
    "    print(model.topics(l=2,n=20), file=f)\n",
    "    \n",
    "#Obtain, rearrange and save the topic distributions for each document\n",
    "doc_topic_mixture=model.get_groups(l=1)['p_tw_d']\n",
    "doc_topic_mixture_L2=model.get_groups(l=2)['p_tw_d']\n",
    "\n",
    "\n",
    "topic_dists_by_doc = [0]*len(titles)\n",
    "for i in range(0,len(titles)):\n",
    "    topic_dists_by_doc[i] = [doc_topic_mixture[j][i] for j in range(0,len(doc_topic_mixture))]\n",
    "    \n",
    "topic_dists_by_doc_L2 = [0]*len(titles)\n",
    "for i in range(0,len(titles)):\n",
    "    topic_dists_by_doc_L2[i] = [doc_topic_mixture_L2[j][i] for j in range(0,len(doc_topic_mixture_L2))]\n",
    "    \n",
    "    \n",
    "np.savetxt('topic_dists_by_doc.txt',topic_dists_by_doc,fmt='%s')\n",
    "np.savetxt('topic_dists_by_doc_L2.txt',topic_dists_by_doc_L2,fmt='%s')\n",
    "\n",
    "\n",
    "#Obtain and save the year associated with each document, and the number of unique years within the corpus. \n",
    "unique_years = set(years)\n",
    "unique_years = list(unique_years)\n",
    "for i in range(0,len(unique_years)):\n",
    "    unique_years[i] = BeautifulSoup(str(unique_years[i]), \"lxml\").text\n",
    "for i in range(0,len(years)):\n",
    "    years[i] = BeautifulSoup(str(years[i]), \"lxml\").text\n",
    "np.savetxt('years.txt',years,fmt='%s')      \n",
    "np.savetxt('unique_years.txt',unique_years,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SBM_KERNEL",
   "language": "python",
   "name": "sbm_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
